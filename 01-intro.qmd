# Introduction {#sec-intro}

```{r}
#| include: false
#| cache: false
source("before-each-chapter.R")
wine_reviews <- fetch_wine_reviews() |> cache("wine_reviews")
```

Data analysis is about finding the stories hidden in the mass of information that make up a data set. Usually we are interested in understanding the major patterns --- the relationships that hold true for most of the data. But sometimes we need to look at the weird observations --- those that don't follow the crowd and behave differently. We call these weird observations "anomalies". They are the mavericks in the data, and in this book, they are what we find interesting.

Anomalies are often a nuisance. For example, they may be simply recording errors that are contaminating our data. We want to find them, remove them, and get on with analysing the bulk of the data. At other times, anomalies are the observations we care about. They tell us things that might otherwise go unnoticed if we only consider how the majority of observations behave. Whether we want to remove them or study them, we first need to find them.

## Definitions

An anomaly is an observation that behaves differently from the bulk of the data. Anomalies are also called "outliers", "novelties", "deviants", "abnormalities" or "discordants". I prefer to use "anomalies" as it is a more general term than outliers, and it is more widely used than the other options.

An old definition due to @Barnett1978 states

> an outlier in a set of data [is] an observation (or subset of observations) which appears to be inconsistent with the remainder of that set of data.

@Hawkins1980 defined it like this:

> An outlier is an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism.

These are too vague to be useful, and we will need to be more specific. Following ???, we define an anomaly as **an observation that has very low probability of occurring under some assumed probability distribution**. This definition is more useful, as it allows us to use probability theory to identify and study anomalies. However, it raises two immediate questions:

  1. What probability distribution should we use? This depends on the application, and the nature of the data. In practice, we will often have to estimate the distribution from the available observations, which leads to further complications that will be discussed later.
  2. What is meant by "very low"? Again, it is application-dependent, and we will need to set an appropriate threshold probability for determining what we consider to be an anomaly.

I will sometimes use the term "outlier" to mean an anomaly that lies at the outer edges of a data set. Other anomalies are actually "inliers", and lie well within the data set. Some papers use the term "inlier" to denote any point that is not an outlier. However, we shall use the earlier definition of "inlier", meaning an observation that is an anomaly but which is surrounded by genuine observations.

### Rejecting anomalies {-}

Many analysts have thrown away apparent anomalies because they were assumed to be errors. There are even formal statistical tests to tell you which observations should be ignored. In my view, this is a dangerous practice. No anomalies should be removed unless you understand what led to their unusual behaviour in the first place. Often the most interesting observations are the ones that appear anomalous. They are the ones that tell us something new and unexpected, so we should not be too quick to reject them.

Sometimes we may want to use methods that are robust to anomalies, so that we can ignore them without worrying about their effect on the analysis. But this is not the same as removing them. Even when we use robust methods, it is still worth investigating any observations that appear anomalous, in order to fully understand the variation in the data.

### How NASA didn't discover the hole in the ozone layer {-}

There is a widespread myth that NASA did not discover the hole in the ozone layer above the Antarctic because they had been throwing away anomalous data that would have revealed it. This is not true, but the real story is also instructive [@Pukelsheim1990;@Christie2001;@christie2004data].

The "ozone hole" refers to low levels of ozone in the atmosphere above Antarctica and surrounding areas. NASA have been collecting satellite data on Antarctic ozone levels using a Total Ozone Mapping Spectrometer (TOMS) since 1979, while British scientists have collected ozone data using ground sensors at the Halley Research Station, on the edge of the Brunt Ice Shelf in Antarctica, since 1957. @fig-halley shows average daily values from the NASA measurements in blue, and from the British observations in orange. There is a clear downward trend in the British data, especially from the late 1970s, which is confirmed with the NASA data. So why wasn't the "ozone hole" discovered until 1985?

```{r}
#| label: fig-halley
#| fig-cap: "Observations of Antarctic ozone levels since 1957, measured in Dobson units (DU). Observations are mean daily values from October each year. Ground observations are from Halley Research Station, while satellite observations were obtained using a Total Ozone Mapping Spectrometer (TOMS). The satellite data were obtained from [Leslie R Lait (NASA)](https://ozonewatch.gsfc.nasa.gov/facts/history_SH.html), while the Halley ground observations were obtained from [Jonathan Shanklin (British Antarctic Survey)](https://www.antarctica.ac.uk/met/jds/ozone/). The horizontal line shows the threshold of 180 DU, used by NASA to determine when the ozone level was unusually low."
#| warning: false
#| message: false
#| fig-height: 3
tmp <- readr::read_csv(here::here("data/halley_toms_ozone.txt"), skip = 2) |>
  rename(
    Year = "#   year",
    Instrument = "instrument",
    Ozone = "ozone"
  )
ozone <- bind_rows(
  tmp |>
    filter(!is.na(Instrument)),
  tmp |>
    filter(is.na(Instrument), !stringr::str_detect(Year, "^#")) |>
    mutate(Instrument = "Halley")
) |>
  mutate(
    Year = readr::parse_integer(Year),
    Ozone = readr::parse_number(Ozone)
  )
ozone |>
  filter(Instrument %in% c("Halley", "TOMS")) |>
  ggplot(aes(x = Year, y = Ozone, color = Instrument)) +
  geom_point() +
  geom_hline(aes(yintercept = 180), col = "gray") +
  labs(y = "Total Ozone (DU)") +
  scale_y_continuous(sec.axis = dup_axis(breaks = 180, name = ""))
```

The British scientists had noticed the low ozone values as early as 1981, but it took a few years for the scientists to be convinced that the low values were real and not due to instrument problems, and then there were the usual publication delays. Eventually, the results were published in @Farman1985.

Meanwhile, NASA was flagging observations as anomalous when they were below 180&nbsp;DU (shown as a horizontal line in @fig-halley). As is clear from the figure, this is much lower than any of the plotted points before the early 1980s. However, the 180 threshold was used for the *daily* measurements, which are much more variable than the monthly averages that are plotted. Occasionally daily observations did fall below 180, and so it was a reasonable threshold for the purpose of identifying instrument problems.

In fact, NASA had checked the unusually low TOMS values obtained before 1985 by comparing them against other available data. But the other data available to them showed ozone values of about 300&nbsp;DU, so it was assumed that the satellite sensor was malfunctioning. The British Halley data were not available to them, and only after the publication of @Farman1985 did the NASA scientists realise that the TOMS results were accurate.

In 1986, NASA scientists were able to confirm the British finding, also demonstrating that the ozone hole was widespread across the Antarctic [@stolarski1986nimbus].

This example reveals some lessons about anomaly detection:

* The NASA threshold of 180 was based on daily data, and was designed to identify instrument problems, not genuine systematic changes in ozone levels. The implicit assumption was that ozone levels varied seasonally, but that otherwise the distribution of observations was stable. All anomaly detection involves some assumptions like this, and it is well to be aware of them. I will aim to make such assumptions explicit in this book.
* Sometimes what we think are anomalies are not really anomalies, but the result of incorrect assumptions.
* Often smoothing or averaging data will help to reveal issues that are not so obvious from the original data. This reduces the variation in the data, and allows more systematic variation to be uncovered.
* Always plot the data. In this case, a graph such as @fig-halley would have revealed the problem in the late 1970s, but it seems no-one was producing plots like this.

## Approaches to anomaly detection

We will take a probabilistic perspective on anomaly detection, where we aim to estimate the likelihood of each observation. Consequently, we will need to introduce (or revise) some material on probability distributions and related tools in @sec-univariate and @sec-multivariate.

### Z-scores and statistical tests

The early days of anomaly detection involved methods based on statistical tests. Typically, the data were assumed to come from some underlying probability distribution, and then observations were declared outliers if they appeared to be inconsistent with this assumption. By far the most popular of these methods are based on the Normal distribution and use z-scores. Even today, such methods are extremely widely used. These are discussed in @sec-tests.

### Boxplot methods

The second half of the 20th century witnessed the advent of computing, which allowed more sophisticated exploratory data analysis to be developed, including boxplots. Most readers will be familiar with boxplots, which provide a simple and effective way to visualise the distribution of a variable, and are often also used for anomaly detection. Such methods are discussed in @sec-boxplots.

### Distance and density-based methods {-}

Modern tools for anomaly detection can be roughly divided into two groups: those based on densities and those based on distances. Distance-based tools compute the pairwise distances between observations and identify anomalies as those points that are far from other points. Density-based tools compute the probability density at each observation and identify anomalies as those points with very low density. The two ideas are closely related because density estimates are often based on distances; for example, a density estimate at a point may be a function of the distances between that point and all other observations. Conversely, distance-based tools are making an implicit assumption about the probability distribution of the data, and how likely it is to find points at different distances from other observations.

Density-based methods are discussed in @sec-density-methods, while distance-based methods are the subject of @sec-distance-methods.

### Statistical vs machine learning approaches {-}

Anomaly detection has a long history in the statistics literature, including all the approaches developed in the 20th century, and many of the newer density-based approaches.

Recently, computer scientists have turned their attention to the problem of anomaly detection, especially as data sets have grown in size and complexity. Anomaly detection arises in cyber-security systems (where intrusions are identified as anomalies), in credit-card fraud (where unusual buying patterns are anomalous) and in remote sensing (where unusual changes in land use are anomalies). Machine-learning approaches have been developed to address these problems, mostly using distance-based methods, although some important contributions to the density-based methods have also appeared in the machine learning literature.

In this book, I make no attempt to distinguish methods as "statistical" or "machine-learning". The distinction is largely about the training of the researchers, or the journals in which they publish, and has little to do with the methods themselves. In any case, the two communities have been slowly moving closer together, with computer scientists taking a more probabilistic perspective than previously, and statisticians taking a more computational and algorithmic approach than their forebears. Now there is now considerable interaction and overlap between these communities, and continuing to label methods as "statistical" or "machine-learning" is unhelpful.

### Supervised vs unsupervised approaches {-}

Some books and papers distinguish "supervised anomaly detection" from "unsupervised anomaly detection". In the former case, anomalies are identified using human input, and then a model or algorithm is employed to learn how to identify new anomalies contained within new data. I do not regard this as anomaly detection, and it will not be covered in this book. It is possible, for example, that what a human labels as "anomalous" is not anomalous in a statistical sense. What is called "supervised anomaly detection" is actually a classification problem, where the aim is to mimic the human who labelled the training set. In that case, different tools are employed.

In this book, we will only consider unsupervised problems. That is, we have no idea *a priori* what observations are anomalous, and there is no "right" answer when it comes to real data.

### Testing anomaly detection algorithms {-}

That makes it difficult to measure how good an anomaly detection algorithm performs. If we don't know the right answer, we can't know if an algorithm has found the true anomalies. There are three solutions to this problem that are often employed when testing anomaly detection algorithms.

1. **Synthetic data**: We can use synthetic data that has been deliberately contaminated with a few known anomalies. This approach allows us to study the sensitivity of an algorithm to anomalies --- how anomalous does an observation have to be before it is detected? A drawback to this approach is that synthetic data is often simpler and neater than real data, making the anomaly detection task easier than it is in reality.
2. **Human labelled**: We can use real data where a human has labelled some of the observations as anomalous. As noted above, this only works when the human has correctly and completely labelled the anomalies. It also requires that what a human considers an anomaly is also anomalous in a statistical sense.
3. **Downsampled category**: We can use real data that contains a categorical variable taking two label values. One of the labelled categories is downsampled and the corresponding observations form the anomalous subset. This approach is useful in obtaining large sets of realistic data, as it takes little time to create the data set. However, it requires that the observations labelled anomalies are different in some way from the other observations (apart from their label).

In this book, we will use data sets of each type when testing different anomaly detection methods. @sec-data discusses the main examples we will be using throughout the book.

## Spurious anomalies and undetected anomalies

**Spurious anomalies** occur when a true observation is identified as anomalous, while **undetected anomalies** occur when an anomalous observation is not detected. Thus, when our purpose is anomaly detection, an undetected anomaly is a "false negative" while a spurious anomaly is a "false positive". We can only really be sure about which observations are spurious or undetected anomalies when the data are synthetic. However, in some real data examples, there are such extreme anomalies that we will proceed under the assumption that we know that these are true anomalies.

In some applications, we will be less interested in detecting anomalies than in ranking observations according to their degree of "anomalousness". For example, we may have a limited team of human analysts who can investigate a few observations, and we want to make sure that they investigate the most anomalous observations. Then it is important to compute an anomaly score, so we can rank the observations according to their scores, and our analysts can look at the observations with the highest scores.

In fact, anomaly scores often underpin the algorithms that are used to detect anomalies. The algorithms compute a score for each observation, and then identify the observations with scores above some threshold as the anomalies. So we will often be talking about anomaly scores, even when we are really interested in detecting anomalies.

## Data sets {#sec-data}

### Test cricket batting data

Cricket is a popular sport in England, India, Australia, and other countries which had strong ties to England during the 19th and 20th centuries. The `cricket_batting` data contains summary statistics for all `r NROW(cricket_batting)` men and women to have played test cricket (the traditional and long form of the game), up to 6 October 2025. Each row contains data on one player, and we will be looking for anomalies in their playing statistics.

```{r}
#| label: cricketdata
#| code-fold: false
cricket_batting
```

### Old Faithful Geyser eruptions {-}

Data on the eruptions of the Old Faithful Geyser in Yellowstone National Park, Wyoming, USA, have been collected for about 150 years. It was named "Old Faithful" due to the relatively predictable timing and length of its eruptions. The `oldfaithful` data set contains data on all `r NROW(oldfaithful)` eruptions recorded between 14 January 2017 and 29 December 2023.

```{r}
#| label: oldfaithfuldata
#| code-fold: false
oldfaithful
```

The time stamp indicates the start time of each eruption, while `recorded_duration` gives what was transcribed by the observer. This has been converted to seconds to give the `duration` variable. The `waiting` variable is the time to the next eruption, again in seconds. Recordings are incomplete, especially during the winter months when observers may not be present.

### Wine prices and quality {-}

The `wine_reviews` data set contains data on `r scales::comma(NROW(wine_reviews))` wines from 44 countries, taken from the *Wine Enthusiast Magazine* during the week of 15 June 2017.

```{r}
#| label: winedata0
#| code-fold: false
#| eval: false
wine_reviews <- fetch_wine_reviews()
```

```{r}
#| label: winedata
#| code-fold: false
wine_reviews
```

The `points` variable provides a measure of wine quality based on a taster's assessment on a scale of 0 to 100. The price is provided in $US. Other variables indicate the location of the winery and year of harvest.

### Synthetic standard normal data

The `n01` data set contains synthetic data on 10 variables, each generated independently from a Normal distribution with mean zero and variance one. So by definition, this data set contains no anomalies. We will add anomalies to the data when testing algorithms, in order to check that the algorithms can identify the artificial anomalies.

```{r}
#| label: n01
#| code-fold: false
n01
```

## Strip plots and scatterplots {#sec-scatterplots}

When there are no more than a few thousand observations, and only one or two variables, it is useful to start with simple plots that allow us to look at the observations directly with no processing or modelling that may hide what is going on.

Let's start with some examples using only one or two variables. These examples will be revisited in the next couple of chapters as we introduce new anomaly detection tools and graphics.

### Example: Don Bradman's batting averages

```{r}
#| label: cricket0
#| echo: false
batting <- cricket_batting |>
  filter(Innings > 20)
donave <- max(batting$Average)
```

Don Bradman was an Australian cricketer in the first half of the 20th century, and is renowned as the best batter to ever play the game. The most common measure of a batter's ability is their career average --- the total number of runs made divided by the number of times they were dismissed. @fig-cricket1 shows a strip plot of the career averages for all `r NROW(batting)` men and women to have played test cricket and batted at least 20 times.

```{r}
#| label: fig-cricket1
#| fig-asp: 0.2
#| fig-cap: !expr paste("Career batting averages for all men and women to have played test cricket and batted at least 20 times. The anomaly is Don Bradman, who averaged", scales::number(donave, 0.01), "over his career.")
cricket_batting |>
  filter(Innings >= 20) |>
  ggplot(aes(x = Average, y = 1)) +
  geom_jitter(width = 0, alpha = 0.5) +
  scale_y_discrete() +
  labs(y = "", x = "Career batting average")
```

The points are "jittered" vertically to reduce overplotting, and made slightly transparent to show where overplotting occurs. There is an obvious anomaly on the right of the plot; this point is Don Bradman who averaged `r scales::number(donave, 0.01)` over his career. Clearly, Don Bradman was much better than any of the other men and women who have played test cricket.

### Example: Old Faithful eruption durations {-}

Some anomalies are not so obvious, and occur within the range of the rest of the data. A good example of this is the duration of eruptions of the Old Faithful Geyser. @fig-oldfaithful1 shows the duration of all eruptions in the data set.

```{r}
#| label: fig-oldfaithful1
#| fig-asp: 0.2
#| fig-cap: "Old Faithful eruption durations between 2017 and 2023."
oldfaithful |>
  ggplot(aes(x = duration, y = 1)) +
  geom_jitter(width = 0, alpha = 0.5) +
  labs(y = "", x = "Duration (seconds)") +
  scale_y_discrete()
```

We see that almost all the eruptions were between 100 and 300 seconds in length, with one extremely short anomalous duration of 1 second, and two other unusually short eruption durations. However, the plot reveals another feature of the data. The majority of observations are between 200 and 300 seconds, with a smaller group at around 120 seconds. Few observations fall between 140 and 180 seconds. Those that do might be considered "inliers" --- anomalous points that lie within the range of the rest of the data but in regions of low density.

The two clusters of observations characterise two eruption modes for the Old Faithful geyser: short durations (around 2 minutes in length) and long durations (between 3 and 5 minutes in length). There was an earthquake in 1998 which changed the distribution of durations. Before 1998, the durations were more evenly split between the two clusters, but currently the long durations are much more common.

The `oldfaithful` data set also contains the time between eruptions. For each row, `waiting` gives the time to the *following* eruption. This is used to predict when the next eruption will happen, based on the duration of the most recent eruption.

```{r}
#| label: fig-oldfaithful3
#| fig-cap: "Old Faithful eruption durations and waiting times between 2017 and 2023."
oldfaithful |>
  ggplot(aes(x = duration, y = waiting)) +
  geom_point(alpha = 0.5) +
  labs(y = "Waiting time to next eruption (seconds)", x = "Duration (seconds)")
```

In @fig-oldfaithful3, we have plotted the duration of each eruption and the waiting time to the next eruption. This shows some anomalies that occur due to the *combination* of the two variables, although they do not appear anomalous within the data on any one of the variables. For example, there are two eruptions of 110--120 seconds, followed by waiting times of over 6000 seconds. Neither the waiting time nor the duration are unusual on their own, but the combination is anomalous. Similarly, there is one eruption of about 240 seconds, followed by a waiting time of about 3000 seconds. The waiting time is the second-smallest overall, but appears even more extraordinary given the long duration of the previous eruption. All other eruptions of similar duration had waiting times nearly twice as long.

### Example: Wine prices and quality {-}

@fig-shiraz shows a scatterplot of data on `r wine_reviews |> filter(variety %in% c("Shiraz", "Syrah")) |> NROW() |> scales::comma()` Syrah wines (also known as Shiraz). The review points provide a measure of the quality of the wine (at least according to one taster's palate). As expected, the price of the wine increases with the quality, although there is considerable variation, and some very expensive wines are rated of relatively low quality, while there are a few exceptional wines at bargain prices.

```{r}
#| label: fig-shiraz
#| fig-cap: "Wine prices and review points for Shiraz and Syrah wines."
wine_reviews |>
  filter(variety %in% c("Shiraz", "Syrah")) |>
  select(points, price) |>
  ggplot(aes(y = price, x = points)) +
  geom_jitter(height = 0, width = 0.3, alpha = 0.5) +
  scale_y_log10()
```

The jittering in the horizontal direction helps reduce overplotting as `points` is always an integer value. This adds a small amount of random noise to the points variable, but not so much that it overlaps with the neighbouring values. The price is shown on a log scale to allow all the points to be seen more clearly.

Here the most interesting observations are the ones that have an unusual price given their points value. For example, the lowest priced wine above 95 points is a 2007 Syrah from the Rulo vineyard in the Columbia Valley, Washington. It is an anomaly given its high points value and low price, although neither the price nor the points value are particularly unusual. There are also two very expensive wines that do not have a rating to match, with points values in the low 90s. These can also be considered anomalies.

```{r}
#| label: shiraz2
#| code-fold: false
wine_reviews |>
  filter(
    variety %in% c("Shiraz", "Syrah"),
    points > 95
  ) |>
  filter(price == min(price)) |>
  select(country, state, region, winery, variety, year, points, price)
```

A similar phenomenon occurs whenever you consider additional variables. An anomaly in three dimensions may not appear anomalous in any of the 2-dimensional sets of variables. As the number of dimensions increases, there are more ways for observations to be anomalous, but it is increasingly difficult to find them.

While strip plots and scatterplots are useful for finding anomalies in small- to moderate-sized data sets with one or two numerical variables, we will need alternative tools once we have three or more variables, or a large data set, or non-numerical data.

We will consider the problem of high dimensions in @sec-highdim, while various types of non-numerical data are considered in the later chapters of the book.

### Example: Synthetic standard normal data

When we have many variables, showing the pairwise scatterplots in a matrix is a useful way to see a lot of information quickly. This is called a scatterplot matrix, and can be produced using the `GGally` package.

@fig-nn01 show the scatterplot matrix of all 10 variables in the `n01` data set. Some transparency has been used to help reduce the overplotting, but with 1000 observations in each small point cloud, it is hard to see any detail other than around the edges.

```{r}
#| label: fig-nn01
#| fig-asp: 1
#| out-width: "100%"
#| message: false
#| fig-cap: "Scatterplots of all pairs of the synthetic `n01` data, where each variable has been generated from independent standard Normal distributions."
n01 |>
  GGally::ggpairs(mapping = aes(alpha = 0.02))
```

Any points that appear slightly outside the main clouds of points are not genuine anomalies, because they were generated from the same N(0,1) distribution as the rest of the data.
