---
title: Bandwidth selection for multivariate kde tuned for anomaly detection
author: Rob J Hyndman
format: pdf
bibliography: weird.bib
biblio-style: authoryear-comp
pdf-engine: pdflatex
include-in-header: preamble.tex
---

```{r}
source("before-each-chapter.R")
```


# Robust covariance estimation

The sample covariance matrix is a useful measure of the spread of a multivariate distribution, given by
$$
\bm{S} = \frac{1}{n-1} \sum_{i=1}^n (\bm{y}_i - \bar{\bm{y}})(\bm{y}_i - \bar{\bm{y}})',
$$ {#eq-cov}
However, it is sensitive to outliers, and so is not suitable for our purposes. There have been many robust estimators of covariance proposed in the literature, but we will discuss only one, relatively simple, estimator known as the "orthogonalized Gnanadesikan/Kettenring" (OGK) estimator [@GK72;@MZ02].

Suppose we have two random variables $X$ and $Y$. Then the variance of their sum and difference is given by
\begin{align*}
  \text{Var}(X+Y) &= \text{Var}(X) + \text{Var}(Y) + 2\text{Cov}(X,Y) \\
  \text{Var}(X-Y) &= \text{Var}(X) + \text{Var}(Y) - 2\text{Cov}(X,Y).
\end{align*}
The difference between these two expressions is
$$
  \text{Var}(X+Y) - \text{Var}(X-Y) = 4\text{Cov}(X,Y),
$$
so that the covariance can be expressed as
$$
\text{Cov}(X,Y) = \frac{1}{4} \left[ \text{Var}(X+Y) - \text{Var}(X-Y)\right].
$$
Now we can use the robust IQR estimate of variance, to estimate the two variances on the right hand side, giving
$$
\hat{s}(X,Y) = \frac{1}{4} \left[ s_{\text{IQR}}^2(X+Y) - s_{\text{IQR}}^2(X-Y)\right].
$$
We can repeat this for each pair of variables, to obtain a robust estimate of the covariance matrix, $\bm{S}^*$. The diagonals can be obtained using the same robust measure of variance. This is known as the Gnanadesikan-Kettenring estimator. The resulting matrix is symmetric, but not necessarily positive definite, which is a requirement of a covariance matrix. So some additional iterative steps are applied to "orthogonalize" it.

1. Compute the eigenvector decomposition of $\bm{S^*}$, so that $\bm{S}^* = \bm{U}\bm{\Lambda}\bm{U}^{-1}$.
2. Project the data onto the basis eigenvectors
3. Estimate the variances (robustly) in the coordinate directions.
4. Then the robust covariance matrix is given by
  $$
    \bm{S}_{\text{OGK}} = \bm{U}\bm{\Lambda}^*\bm{U}^{-1},
  $$ {#eq-ogk}
  where $\bm{\Lambda}^*$ is a diagonal matrix with the robust variances on the diagonal.

These orthogonalization steps are usually repeated one more time.

This procedure is implemented in the `covOGK` function in the `robustbase` package [@robustbase].

# Multivariate kernel density estimation

Suppose our observations are $d$-dimensional vectors, $\bm{y}_1,\dots,\bm{y}_n$. Then the multivariate kernel density estimate is given by [@Scott2015]
$$
  \hat{f}(\bm{y}) = \frac{1}{n} \sum_{i=1}^n K_H(\bm{y} - \bm{y}_i),
$$ {#eq-mkde}
where $K_H$ is a multivariate probability density with covariance matrix $\bm{H}$. For example, a multivariate Gaussian kernel is given by
$$
  K_H(\bm{u}) = (2\pi)^{-d/2} |\bm{H}|^{-1/2} \exp \{-\textstyle\frac12 \bm{u}'\bm{H}^{-1}\bm{u} \}.
$$

## Bandwidth matrix selection

The optimal bandwidth matrix (minimizing the mean integrated squared error between the true density and its estimate) is of the order $n^{-2/(d+4)}$. If such a bandwidth matrix is used, then the estimator converges at rate $n^{-4/(d+4)}$, implying that kernel density estimation becomes increasingly difficult as the dimension $d$ increases. This is to be expected given the curse of dimensionality, as the number of observations required to obtain a good estimate increases exponentially with the dimension. In practice, we rarely attempt to estimate a density in more than $d=3$ dimensions.

If the underlying density is Normal with mean $\bm{\mu}$ and variance $\bm{\Sigma}$, then the optimal bandwidth matrix is given by
$$
  \bm{H} = \left(\frac{4}{d+2}\right)^{2/(d+4)} n^{-2/(d+4)} \bm{\Sigma}.
$$ {#eq-gaussianH}

Replacing $\bm{\Sigma}$ by the robust covariance matrix $\bm{S}_{\text{OGK}}$ (@eq-ogk), we obtain a robust normal reference rule.

This is optimal for densities with the same curvature as a normal density, and will probably still give a good estimate for other densities as it is consistent for any smooth $f$.

This provides a good estimate of the overall density (as it minimizes the MISE), but it is not particularly good in the tails of the distribution. Instead we usually need a larger density matrix.

We propose choosing
$$
\bm{H} = k_n \bm{S}_{\text{OGK}},
$$
where $k_n$ is determined by controlling the false anomaly rate for a multivariate Gaussian distribution.


```{r}
library(weird)
library(mvtnorm)
d <- 2
nreps <- 500
sim_kn <- tidyr::expand_grid(
  n = c(100, 200, 500, 1000, 2000, 5000),
  kn = c(1, 2, 3, 5, 10),
  nfalse = NA
)
sim_tda <- tibble(
  n = unique(sim_kn$n),
  nfalse = NA
)
no_false <- any_false <- rep(NA, nreps)
for (i in seq(NROW(sim_kn))) {
  cat(paste("n =", sim_kn$n[i], "kn =", sim_kn$kn[i], "\n"))
  for (j in seq(nreps)) {
    y <- rmvnorm(sim_kn$n[i], sigma = diag(d))
    H <- kde_bandwidth(y, multiplier = sim_kn$kn[i])
    scores <- weird:::calc_kde_scores(y, H = H)
    lookout_prob <- lookout(density_scores = scores$scores, loo_scores = scores$loo_scores)
    # Number of false anomalies
    no_false[j] <- sum(lookout_prob < 0.05)
  }
  # Average number of false anomalies
  sim_kn$nfalse[i] <- mean(no_false)
}
for (i in seq(NROW(sim_tda))) {
  cat(paste("n =", sim_tda$n[i], "\n"))
  for (j in seq(nreps)) {
    y <- rmvnorm(sim_tda$n[i], sigma = diag(d))
    H <- kde_bandwidth(y, method = "lookout")
    scores <- weird:::calc_kde_scores(y, H = H)
    lookout_prob <- lookout(density_scores = scores$scores, loo_scores = scores$loo_scores)
    # Number of false anomalies
    no_false[j] <- sum(lookout_prob < 0.05)
  }
  # Average number of false anomalies
  sim_tda$nfalse[i] <- mean(no_false)
}

# Plot proportion of false anomalies vs n
sim_kn |>
  mutate(kn = as.character(kn)) |>
  bind_rows(
    sim_tda |> mutate(kn = "lookout")
  ) |>
  mutate(
    proportion = nfalse / n,
    kn = factor(kn, levels = c("1", "2", "3", "5", "10", "lookout"))
  ) |>
  ggplot(aes(x = n, y = proportion)) +
  geom_line(aes(col = kn, group = kn)) +
  scale_y_log10()
```


## Some examples

Old Faithful data

```{r}
of <- oldfaithful |>
  select(duration, waiting)
gg_hdrboxplot(of, duration, waiting,
  show_points = TRUE,
  H = kde_bandwidth(of, multiplier = 3)
)
```

Cricket batting averages

```{r}
#| fig-height: 2
bat_ave <- cricket_batting |>
  filter(Innings > 20)
bat_ave |>
  gg_hdrboxplot(Average,
    show_points = TRUE,
    h = kde_bandwidth(bat_ave$Average, multiplier = 3)
  )
```

# References
